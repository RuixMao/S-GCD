{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuixMao/S-GCD/blob/main/TrialsForGCD_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salinas"
      ],
      "metadata": {
        "id": "m3sPyf7zoKCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuLRuu7HMX3K",
        "outputId": "b29f2bd3-7dee-4460-a80f-ac463c52c05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-9\n",
            "8373\n",
            "[[   0 1790]\n",
            " [   1  391]\n",
            " [  10 1343]\n",
            " [  11  616]\n",
            " [  12 1525]\n",
            " [  13  674]\n",
            " [  14  799]]\n",
            "   \n",
            "(83, 86, 204)\n",
            "83  x  86  =  7138\n",
            "(3484, 204)\n",
            "(3484,)\n",
            "  \n",
            "(2787, 204)\n",
            "(2561, 204)\n",
            "2787\n",
            "697\n",
            "1864\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['salinasA_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['salinasA_gt']\n",
        "\n",
        "# https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes\n",
        "data = load_data('/content/SalinasA_corrected.mat')\n",
        "gt_data = load_gt('/content/SalinasA_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_classes = [10, 11, 12]\n",
        "unknown_classes = [1, 13, 14]\n",
        " # we need all classes without 0 class (because 0 is not class label)\n",
        "all_classes = np.concatenate((known_classes,unknown_classes), axis=0)\n",
        "\n",
        "all_data = flattened_data[np.isin(flattened_gt, all_classes)]\n",
        "all_labels = flattened_gt[np.isin(flattened_gt, all_classes)]\n",
        "\n",
        "known_data = all_data[np.isin(all_labels, known_classes)]\n",
        "known_labels = all_labels[np.isin(all_labels, known_classes)]\n",
        "unknown_data = all_data[np.isin(all_labels, unknown_classes)]\n",
        "unknown_labels = all_labels[np.isin(all_labels, unknown_classes)]\n",
        "\n",
        "# PCA: before OR after the splitting?\n",
        "# In this case: after the splitting\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled,unknown_data), axis=0)\n",
        "y_test = np.concatenate((y_test_labeled,unknown_labels), axis=0)\n",
        "\n",
        "trainN=len(y_train)\n",
        "testN1=len(y_test_labeled)\n",
        "testN2=len(unknown_labels)\n",
        "\n",
        "print(min_value)\n",
        "print(max_value)\n",
        "unique, counts = np.unique(flattened_gt, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)\n",
        "print(\"   \")\n",
        "print(data.shape)\n",
        "print(data.shape[0],\" x \", data.shape[1], \" = \", data.shape[0]*data.shape[1])\n",
        "print(known_data.shape)\n",
        "print(known_labels.shape)\n",
        "print(\"  \")\n",
        "print(X_train_origin.shape)\n",
        "print(X_test_origin.shape)\n",
        "print(trainN)\n",
        "print(testN1)\n",
        "print(testN2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=30)  # are the results worse without PCA?\n",
        "pca.fit(X_train_origin)\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)  # Transform all data using PCA\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnASat34Sb3-",
        "outputId": "1611ba6e-4636-413a-8026-19b70007ebaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2787, 30)\n",
            "(2561, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spectral Clustering with 6 clusters (adjust number of clusters if needed)\n",
        "clustering = SpectralClustering(n_clusters=6, assign_labels='kmeans', random_state=42)\n",
        "# clustering = KMeans(n_clusters=6, random_state=42)\n",
        "all_data_pca = np.vstack([X_train, X_test])\n",
        "all_cluster_labels = clustering.fit_predict(all_data_pca)\n",
        "\n",
        "unique, counts = np.unique(all_cluster_labels, return_counts=True)\n",
        "print(unique, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUq0x2yleiGx",
        "outputId": "ab9b44e4-7f88-4769-d9af-21389dc16a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5] [ 601 1360  390 1579  883  535]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign all_cluster_labels with class_labels in X_train\n",
        "import statistics\n",
        "from statistics import mode\n",
        "\n",
        "all_cluster_labels_train = all_cluster_labels[:trainN]\n",
        "all_cluster_labels_test = all_cluster_labels[trainN:]\n",
        "\n",
        "all_cluster_labels_a = all_cluster_labels_train[np.isin(y_train, known_classes[0])]\n",
        "all_cluster_labels_b = all_cluster_labels_train[np.isin(y_train, known_classes[1])]\n",
        "all_cluster_labels_c = all_cluster_labels_train[np.isin(y_train, known_classes[2])]\n",
        "print(len(all_cluster_labels_a))\n",
        "print(len(all_cluster_labels_b))\n",
        "print(len(all_cluster_labels_c))\n",
        "\n",
        "# print(statistics.mode(all_cluster_labels_a))\n",
        "# print(statistics.mode(all_cluster_labels_b))\n",
        "# print(statistics.mode(all_cluster_labels_c))\n",
        "\n",
        "# Statistics of Class1, Class10, Class11 with different clusters:\n",
        "# print(np.unique(all_cluster_labels_a, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_b, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_c, return_counts=True))\n",
        "\n",
        "AssignMatrix=np.zeros((3, 6))\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)\n",
        "print(AssignMatrix)\n",
        "\n",
        "# Instead of counts the relative ratio is better:\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)/len(all_cluster_labels_a)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)/len(all_cluster_labels_b)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)/len(all_cluster_labels_c)\n",
        "print(AssignMatrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQsAkWuSqLSu",
        "outputId": "0c0af270-f719-4ae4-9037-61a4682999a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1085\n",
            "484\n",
            "1218\n",
            "[[   2.  633.    0.   13.    8.  429.]\n",
            " [   0.  459.    0.   25.    0.    0.]\n",
            " [   0.    0.    0. 1218.    0.    0.]]\n",
            "[[0.00184332 0.58341014 0.         0.01198157 0.00737327 0.39539171]\n",
            " [0.         0.94834711 0.         0.05165289 0.         0.        ]\n",
            " [0.         0.         0.         1.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hungarian method for best (maximized) assignment\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "row_ind, col_ind = linear_sum_assignment(AssignMatrix, maximize='true')\n",
        "print(col_ind)\n",
        "\n",
        "# col_ind[0] is for class1, col_ind[1] is for class10, col_ind[2] is for class11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2oKoZgWLKwP",
        "outputId": "83308303-c366-41de-f220-f79db184afa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 1 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. method\n",
        "# Using only clustering and assigment between the known classes and clusters\n",
        "# inference: use the cluster ID for classification and clustering\n",
        "# instance in X_test is predicted based on the instance's cluster ID\n",
        "# E.g.:\n",
        "# if 0 then this belongs to Cluster0\n",
        "# if 1 then this belongs to Class1\n",
        "# if 2 then this belongs to Class11\n",
        "# if 3 then this belongs to Cluster3\n",
        "# if 4 then this belongs to Cluster4\n",
        "# if 5 then this belongs to Class10\n",
        "\n",
        "i = 0\n",
        "while i < len(all_cluster_labels):\n",
        "  if all_cluster_labels[i] == col_ind[0]:\n",
        "    all_cluster_labels[i] = known_classes[0]\n",
        "  if all_cluster_labels[i] == col_ind[1]:\n",
        "    all_cluster_labels[i] = known_classes[1]\n",
        "  if all_cluster_labels[i] == col_ind[2]:\n",
        "    all_cluster_labels[i] = known_classes[2]\n",
        "  i += 1\n",
        "\n",
        "y_test_labeled_Predicted=all_cluster_labels[trainN:trainN+testN1]\n",
        "unknown_labels_Predicted=all_cluster_labels[trainN+testN1:trainN+testN1+testN2]\n",
        "print(len(y_test_labeled_Predicted))\n",
        "print(len(unknown_labels_Predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_daqpPJUe1ki",
        "outputId": "9ce9d5e5-46e1-48ae-d828-670a74ae58c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "697\n",
            "1864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. method\n",
        "# Use an extended classifier, where the rest of clusters added as dummy classes\n",
        "# a. Training a classifier with:\n",
        "#    Train set with ground true labels + Subset of Test set (belonging to rest clusters)\n",
        "# b. Test phase of the classifier, where inference is the same as in 1. method\n",
        "\n",
        "rest_clusters=list(range(len(all_classes)))\n",
        "for i in range (len(col_ind)):\n",
        "  rest_clusters.remove(col_ind[i])\n",
        "print(rest_clusters)\n",
        "\n",
        "X_test_subset=X_test[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "y_test_subset=y_test[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = svm_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:testN1]\n",
        "unknown_labels_Predicted2 = y_test_pred2[testN1:]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GdcAgbov6SV",
        "outputId": "28072e83-7ca5-43aa-a6c0-1aa9f70746f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 4]\n",
            "697\n",
            "1864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# Evaluation of the methods\n",
        "# metrics from https://www.geeksforgeeks.org/clustering-performance-evaluation-in-scikit-learn/\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score, rand_score, fowlkes_mallows_score, silhouette_score, davies_bouldin_score, mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Results of the 1. method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\" \")\n",
        "print(\"Results of the 2. method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted2))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wDMfsfdyljO",
        "outputId": "894efcc0-0c0d-48f7-dfd3-07e5f1e97d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of the 1. method\n",
            "accuracy:  0.7618364418938307\n",
            "rand index:  0.8533481232678845\n",
            "adjusted rand index:  0.6824048749840209\n",
            "fowlkes mallows :  0.7972981609048793\n",
            " \n",
            "Results of the 2. method\n",
            "accuracy:  0.975609756097561\n",
            "rand index:  0.9878720232952988\n",
            "adjusted rand index:  0.9735827585422115\n",
            "fowlkes mallows :  0.9830182889766578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. method\n",
        "# Let us calculate the centroids of the classes in X_train\n",
        "\n",
        "# 3.1 method: Let us calculate the centroids of the rest clusters\n",
        "# 3.2 method: Let us calculate the centroids of the rest clusters only with X_test\n",
        "# (so at 3.2. we should remove the X_train instances in the rest clusters)\n",
        "\n",
        "# Centroids for known classes (you've already done this)\n",
        "X_train_a_mean = np.mean(X_train[np.isin(y_train, known_classes[0])], axis=0)\n",
        "X_train_b_mean = np.mean(X_train[np.isin(y_train, known_classes[1])], axis=0)\n",
        "X_train_c_mean = np.mean(X_train[np.isin(y_train, known_classes[2])], axis=0)\n",
        "\n",
        "# 3.1 method: Calculate the centre of mass of rest clusters using combined data\n",
        "rest_clusters_centroids = []\n",
        "\n",
        "# Create a boolean mask for trainN+testN1 based on the shape of all_cluster_labels\n",
        "# First, filter out the required data based on trainN+testN1\n",
        "relevant_data_combined = all_data_combined[:trainN+testN1]\n",
        "\n",
        "for cluster in rest_clusters:\n",
        "    mask_cluster = (all_cluster_labels[:trainN+testN1] == cluster)\n",
        "    rest_cluster_data = relevant_data_combined[mask_cluster]\n",
        "    if rest_cluster_data.size > 0:\n",
        "        rest_cluster_mean = np.mean(rest_cluster_data, axis=0)\n",
        "        rest_clusters_centroids.append(rest_cluster_mean)\n",
        "\n",
        "\n",
        "\n",
        "# 3.2 method: Only use X_test data to calculate center of mass\n",
        "# Assume all_cluster_labels_test is the label set for X_test\n",
        "rest_clusters_centroids_test_only = []\n",
        "for cluster in rest_clusters:\n",
        "    rest_cluster_data = X_test[all_cluster_labels_test == cluster]\n",
        "    if rest_cluster_data.size > 0:\n",
        "        rest_cluster_mean = np.mean(rest_cluster_data, axis=0)\n",
        "        rest_clusters_centroids_test_only.append(rest_cluster_mean)\n",
        "\n",
        "# Distance calculation for each instance in X_test to centroids and decide the nearest one\n",
        "all_centroids_3_1 = [X_train_a_mean, X_train_b_mean, X_train_c_mean] + rest_clusters_centroids\n",
        "all_centroids_3_2 = [X_train_a_mean, X_train_b_mean, X_train_c_mean] + rest_clusters_centroids_test_only\n",
        "\n",
        "def nearest_centroid_indices(X, centroids):\n",
        "    nearest = []\n",
        "    for instance in X:\n",
        "        distances = [np.linalg.norm(instance - centroid) for centroid in centroids]\n",
        "        nearest.append(np.argmin(distances))\n",
        "    return nearest\n",
        "\n",
        "nearest_to_3_1 = nearest_centroid_indices(X_test, all_centroids_3_1)\n",
        "nearest_to_3_2 = nearest_centroid_indices(X_test, all_centroids_3_2)\n",
        "\n",
        "print(nearest_to_3_1)  # Print the closest centre-of-mass index for each test instance using method 3.1\n",
        "print(nearest_to_3_2)  # Print the closest centre-of-mass index for each test instance using method 3.2\n",
        "\n",
        "# Alternative solution: X_train_a_mean = [float(sum(i))/len(i) for i in zip(*X_train_a)]\n",
        "\n",
        "print(X_train_a.shape)\n",
        "print(X_train_b.shape)\n",
        "print(X_train_c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJhO-7FUylR_",
        "outputId": "f9f4b6d5-2e3f-4b25-be97-9c37b3c3412e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 4, 2, 1, 1, 1, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 0, 2, 2, 2, 2, 1, 2, 0, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 0, 1, 2, 2, 1, 2, 1, 0, 1, 2, 2, 4, 2, 2, 2, 0, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 2, 1, 1, 2, 1, 0, 0, 2, 4, 2, 1, 2, 1, 2, 2, 0, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2, 2, 1, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 0, 1, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 0, 2, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 0, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 4, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 0, 0, 2, 1, 1, 2, 1, 1, 4, 2, 1, 1, 2, 2, 2, 0, 1, 2, 2, 2, 1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 2, 1, 1, 2, 4, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 1, 2, 2, 2, 0, 1, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1, 2, 0, 2, 1, 2, 0, 0, 0, 1, 0, 0, 2, 1, 0, 2, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 2, 0, 2, 0, 1, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 1, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 1, 0, 1, 1, 2, 1, 2, 0, 1, 2, 0, 2, 1, 1, 1, 2, 2, 1, 2, 0, 1, 0, 1, 1, 2, 0, 0, 1, 2, 2, 2, 1, 2, 0, 1, 1, 2, 0, 1, 1, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 1, 4, 1, 1, 1, 0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 2, 1, 0, 2, 1, 2, 0, 1, 0, 1, 2, 0, 1, 1, 0, 4, 2, 1, 2, 2, 1, 1, 1, 0, 1, 2, 4, 0, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 4, 1, 2, 2, 2, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 1, 4, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 0, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 0, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3]\n",
            "[0, 1, 2, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 5, 2, 1, 1, 1, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 0, 2, 2, 2, 2, 1, 2, 0, 5, 2, 2, 0, 1, 1, 1, 2, 2, 5, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 0, 1, 2, 2, 1, 2, 1, 5, 1, 2, 2, 5, 2, 2, 2, 5, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 5, 1, 1, 5, 2, 0, 1, 5, 1, 2, 1, 1, 2, 1, 0, 5, 2, 3, 2, 1, 2, 1, 2, 2, 0, 2, 1, 2, 1, 1, 1, 1, 0, 5, 1, 1, 1, 1, 2, 1, 2, 2, 1, 5, 2, 1, 2, 2, 2, 2, 5, 2, 2, 1, 1, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 5, 1, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 5, 2, 1, 0, 1, 5, 2, 1, 1, 0, 0, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 5, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 5, 2, 5, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 0, 5, 2, 1, 1, 2, 1, 1, 5, 2, 1, 1, 2, 2, 2, 5, 1, 2, 2, 2, 1, 2, 5, 1, 2, 2, 1, 5, 1, 0, 2, 2, 1, 1, 2, 5, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 5, 2, 2, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 5, 5, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 5, 5, 1, 2, 2, 1, 2, 2, 2, 0, 1, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1, 2, 0, 2, 1, 2, 0, 0, 5, 1, 0, 5, 2, 1, 0, 2, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 3, 2, 2, 0, 2, 5, 1, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 1, 2, 0, 2, 0, 2, 5, 2, 2, 2, 5, 1, 0, 1, 1, 2, 1, 2, 0, 1, 2, 5, 2, 1, 1, 1, 2, 2, 1, 2, 5, 1, 0, 1, 1, 2, 5, 0, 1, 2, 2, 2, 1, 2, 5, 1, 1, 2, 5, 1, 1, 2, 1, 0, 0, 2, 5, 5, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 1, 5, 1, 1, 1, 5, 2, 2, 1, 5, 0, 5, 1, 0, 5, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 2, 1, 0, 2, 1, 2, 5, 1, 0, 1, 2, 5, 1, 1, 5, 5, 2, 1, 2, 2, 1, 1, 1, 0, 1, 2, 5, 0, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 5, 5, 1, 2, 2, 2, 5, 0, 1, 1, 1, 1, 5, 2, 2, 2, 2, 1, 5, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 0, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 5, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 0, 0, 5, 5, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 0, 0, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 0, 0, 0, 5, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "(1085, 30)\n",
            "(484, 30)\n",
            "(1218, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score, rand_score, fowlkes_mallows_score, silhouette_score, davies_bouldin_score, mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Results of the 3. method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwBWfEjNPXGK",
        "outputId": "87da9e64-be39-4efd-f07a-3737782d5060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of the 3. method\n",
            "accuracy:  0.7618364418938307\n",
            "rand index:  0.8533481232678845\n",
            "adjusted rand index:  0.6824048749840209\n",
            "fowlkes mallows :  0.7972981609048793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.method"
      ],
      "metadata": {
        "id": "D8Xw94IwXGxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sgvaze/generalized-category-discovery.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4iublFaXIAV",
        "outputId": "b808c762-2345-4fb9-ade3-d117bae35d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'generalized-category-discovery'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 76 (delta 13), reused 53 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (76/76), 3.70 MiB | 8.56 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv generalized-category-discovery gcd"
      ],
      "metadata": {
        "id": "Fub6ayczXLhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "zmLW0uCxXQXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. method\n",
        "# semi-supervised k-means clustering\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import normalized_mutual_info_score as nmi_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statistics import mode\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you've already imported other necessary modules like clustering, etc.\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['salinasA_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['salinasA_gt']\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(labeled_data)\n",
        "labeled_data_pca = pca.transform(labeled_data)\n",
        "unlabeled_data_pca = pca.transform(unlabeled_data)\n",
        "min_length = min(len(unlabeled_data_pca), len(labeled_data_pca))\n",
        "unlabeled_data_pca = unlabeled_data_pca[:min_length]\n",
        "labeled_data_pca = labeled_data_pca[:min_length]\n",
        "\n",
        "# PCA transformation\n",
        "unlabeled_data_pca = pca.transform(X_test_origin[len(y_test_labeled):])\n",
        "labeled_data_pca = X_train\n",
        "y_train_true = y_train\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "unlabeled_data_pca = torch.from_numpy(unlabeled_data_pca).to(device)\n",
        "labeled_data_pca = torch.from_numpy(labeled_data_pca).to(device)\n",
        "y_train_true = torch.tensor(y_train_true).to(device)\n",
        "\n",
        "# Assuming you have an implementation for SemiSupKMeans:\n",
        "semi_sup_kmeans = SemiSupKMeans(k=8, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit_mix(unlabeled_data_pca, labeled_data_pca, y_train_true)\n",
        "\n",
        "# Get cluster labels for training data\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "y_train_pred = np.empty_like(y_train_true.cpu().numpy())\n",
        "y_test_pred = np.empty_like(y_test_true)\n",
        "\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "for cluster in unique_clusters:\n",
        "    mask_train = cluster_labels[:len(y_train_true)] == cluster\n",
        "    mask_test = cluster_labels[len(y_train_true):] == cluster\n",
        "    common_label = mode(y_train_true.cpu().numpy()[mask_train]) if mask_train.sum() > 0 else -1\n",
        "    y_train_pred[mask_train] = common_label\n",
        "    y_test_pred[mask_test] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true.cpu().numpy(), y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "print(f'Testing accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine true labels of training and testing sets\n",
        "y_true_combined = np.concatenate((y_train_true.cpu().numpy(), y_test_true))\n",
        "y_pred_combined = np.concatenate((y_train_pred, y_test_pred))\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "print(f'Overall accuracy: {overall_accuracy * 100:.2f}%')\n",
        "\n",
        "# Known label comparison visualization\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(labeled_data_pca.cpu().numpy()[:, 0], labeled_data_pca.cpu().numpy()[:, 1], c=y_train_true.cpu().numpy(), cmap='rainbow', marker='o', label='True Labels')\n",
        "plt.scatter(labeled_data_pca.cpu().numpy()[:, 0], labeled_data_pca.cpu().numpy()[:, 1], c=y_train_pred, cmap='rainbow', marker='x', label='Predicted Labels')\n",
        "plt.title('Known Label Comparison (True label vs Predicted label)')\n",
        "plt.legend()\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oLGFkm132WpG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "9d6b1128-15be-4486-9d81-852f3cc42800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-20184e355e09>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# PCA for dimensionality reduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mlabeled_data_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0munlabeled_data_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labeled_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indian Pines"
      ],
      "metadata": {
        "id": "OV8vdS_goOkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kmeans / SVM+Kmeans"
      ],
      "metadata": {
        "id": "w1zM9_2gDCMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    print(data.keys())  # Add this line\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "# https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_classes = [2, 4, 5, 8, 9, 10, 13, 14]\n",
        "unknown_classes = [1, 3, 6, 7, 11, 12, 15, 16]\n",
        " # we need all classes without 0 class (because 0 is not class label)\n",
        "all_classes = np.concatenate((known_classes,unknown_classes), axis=0)\n",
        "\n",
        "all_data = flattened_data[np.isin(flattened_gt, all_classes)]\n",
        "all_labels = flattened_gt[np.isin(flattened_gt, all_classes)]\n",
        "\n",
        "known_data = all_data[np.isin(all_labels, known_classes)]\n",
        "known_labels = all_labels[np.isin(all_labels, known_classes)]\n",
        "unknown_data = all_data[np.isin(all_labels, unknown_classes)]\n",
        "unknown_labels = all_labels[np.isin(all_labels, unknown_classes)]\n",
        "\n",
        "# PCA: before OR after the splitting?\n",
        "# In this case: after the splitting\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled,unknown_data), axis=0)\n",
        "y_test = np.concatenate((y_test_labeled,unknown_labels), axis=0)\n",
        "\n",
        "trainN=len(y_train)\n",
        "testN1=len(y_test_labeled)\n",
        "testN2=len(unknown_labels)\n",
        "\n",
        "print(min_value)\n",
        "print(max_value)\n",
        "unique, counts = np.unique(flattened_gt, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)\n",
        "print(\"   \")\n",
        "print(data.shape)\n",
        "print(data.shape[0],\" x \", data.shape[1], \" = \", data.shape[0]*data.shape[1])\n",
        "print(known_data.shape)\n",
        "print(known_labels.shape)\n",
        "print(\"  \")\n",
        "print(X_train_origin.shape)\n",
        "print(X_test_origin.shape)\n",
        "print(trainN)\n",
        "print(testN1)\n",
        "print(testN2)"
      ],
      "metadata": {
        "id": "rtu95s7hPur9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743057a7-00ea-4a8b-ffaf-52a00d221b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
            "955\n",
            "9604\n",
            "[[    0 10776]\n",
            " [    1    46]\n",
            " [    2  1428]\n",
            " [    3   830]\n",
            " [    4   237]\n",
            " [    5   483]\n",
            " [    6   730]\n",
            " [    7    28]\n",
            " [    8   478]\n",
            " [    9    20]\n",
            " [   10   972]\n",
            " [   11  2455]\n",
            " [   12   593]\n",
            " [   13   205]\n",
            " [   14  1265]\n",
            " [   15   386]\n",
            " [   16    93]]\n",
            "   \n",
            "(145, 145, 200)\n",
            "145  x  145  =  21025\n",
            "(5088, 200)\n",
            "(5088,)\n",
            "  \n",
            "(4070, 200)\n",
            "(6179, 200)\n",
            "4070\n",
            "1018\n",
            "5161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=30)  # are the results worse without PCA?\n",
        "pca.fit(X_train_origin)\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)  # Transform all data using PCA\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "SlVxZx-SPu5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b479f41-dffc-4f43-c750-7758417e6cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4070, 30)\n",
            "(6179, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spectral Clustering with 16 clusters (adjust number of clusters if needed)\n",
        "#clustering = SpectralClustering(n_clusters=6, assign_labels='kmeans', random_state=42)\n",
        "clustering = KMeans(n_clusters=16, random_state=42)\n",
        "all_data_pca = np.vstack([X_train, X_test])\n",
        "all_cluster_labels = clustering.fit_predict(all_data_pca)\n",
        "\n",
        "unique, counts = np.unique(all_cluster_labels, return_counts=True)\n",
        "print(unique, counts)"
      ],
      "metadata": {
        "id": "i3I1UNMjPvBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bc7839-0bfc-4e6b-db21-a3e75c1b975c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] [ 847  467  799  496 1478  177  551  451  453   78 1573  750  624  594\n",
            "  511  400]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign all_cluster_labels with class_labels in X_train\n",
        "import statistics\n",
        "from statistics import mode\n",
        "\n",
        "all_cluster_labels_train = all_cluster_labels[:trainN]\n",
        "all_cluster_labels_test = all_cluster_labels[trainN:]\n",
        "\n",
        "all_cluster_labels_a = all_cluster_labels_train[np.isin(y_train, known_classes[0])]\n",
        "all_cluster_labels_b = all_cluster_labels_train[np.isin(y_train, known_classes[1])]\n",
        "all_cluster_labels_c = all_cluster_labels_train[np.isin(y_train, known_classes[2])]\n",
        "print(len(all_cluster_labels_a))\n",
        "print(len(all_cluster_labels_b))\n",
        "print(len(all_cluster_labels_c))\n",
        "\n",
        "# print(statistics.mode(all_cluster_labels_a))\n",
        "# print(statistics.mode(all_cluster_labels_b))\n",
        "# print(statistics.mode(all_cluster_labels_c))\n",
        "\n",
        "# Statistics of Class1, Class10, Class11 with different clusters:\n",
        "# print(np.unique(all_cluster_labels_a, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_b, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_c, return_counts=True))\n",
        "\n",
        "AssignMatrix=np.zeros((3, 6))\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)\n",
        "print(AssignMatrix)\n",
        "\n",
        "# Instead of counts the relative ratio is better:\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)/len(all_cluster_labels_a)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)/len(all_cluster_labels_b)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)/len(all_cluster_labels_c)\n",
        "print(AssignMatrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bto1ae84oegC",
        "outputId": "5c12ed16-bf5f-46a4-d14c-fc2a9a2ed064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1152\n",
            "185\n",
            "385\n",
            "[[105.   0. 240.   1. 194.   8.]\n",
            " [  1.   0.  25.   0.   4.   0.]\n",
            " [  0.   6.   0.  33.   1.   0.]]\n",
            "[[0.09114583 0.         0.20833333 0.00086806 0.16840278 0.00694444]\n",
            " [0.00540541 0.         0.13513514 0.         0.02162162 0.        ]\n",
            " [0.         0.01558442 0.         0.08571429 0.0025974  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hungarian method for best (maximized) assignment\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "row_ind, col_ind = linear_sum_assignment(AssignMatrix, maximize='true')\n",
        "print(col_ind)\n",
        "\n",
        "# col_ind[0] is for class1, col_ind[1] is for class10, col_ind[2] is for class11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZBSD4Kcoeih",
        "outputId": "f4164da2-70fb-46e6-f306-26c0b2046cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. method\n",
        "# Using only clustering and assigment between the known classes and clusters\n",
        "# inference: use the cluster ID for classification and clustering\n",
        "# instance in X_test is predicted based on the instance's cluster ID\n",
        "# E.g.:\n",
        "# if 0 then this belongs to Cluster0\n",
        "# if 1 then this belongs to Class1\n",
        "# if 2 then this belongs to Class11\n",
        "# if 3 then this belongs to Cluster3\n",
        "# if 4 then this belongs to Cluster4\n",
        "# if 5 then this belongs to Class10\n",
        "\n",
        "# Use KMeans clustering on the training data\n",
        "n_clusters = len(known_classes)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(X_train_origin)\n",
        "\n",
        "# Predict cluster IDs for all data\n",
        "all_cluster_labels = kmeans.predict(all_data)\n",
        "\n",
        "# A mapping between cluster IDs and known classes\n",
        "# This mapping needs to be determined based on domain knowledge or some logic\n",
        "# Here's a dummy mapping as an example:\n",
        "col_ind = [0, 1, 2]  # map cluster 0 to known_classes[0], cluster 1 to known_classes[1], etc.\n",
        "\n",
        "i = 0\n",
        "while i < len(all_cluster_labels):\n",
        "    for j in range(len(col_ind)):\n",
        "        if all_cluster_labels[i] == col_ind[j]:\n",
        "            all_cluster_labels[i] = known_classes[j]\n",
        "    i += 1\n",
        "\n",
        "y_test_labeled_Predicted = all_cluster_labels[trainN:trainN+testN1]\n",
        "unknown_labels_Predicted = all_cluster_labels[trainN+testN1:trainN+testN1+testN2]\n",
        "\n",
        "print(len(y_test_labeled_Predicted))\n",
        "print(len(unknown_labels_Predicted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apjyy1TGoekv",
        "outputId": "69960cb8-3a03-47df-db70-07f72ec06f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1358\n",
            "3463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. method\n",
        "# Use an extended classifier, where the rest of clusters added as dummy classes\n",
        "# a. Training a classifier with:\n",
        "#    Train set with ground true labels + Subset of Test set (belonging to rest clusters)\n",
        "# b. Test phase of the classifier, where inference is the same as in 1. method\n",
        "\n",
        "rest_clusters=list(range(len(all_classes)))\n",
        "for i in range (len(col_ind)):\n",
        "  rest_clusters.remove(col_ind[i])\n",
        "print(rest_clusters)\n",
        "\n",
        "X_test_subset=X_test[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "y_test_subset=y_test[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = svm_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:testN1]\n",
        "unknown_labels_Predicted2 = y_test_pred2[testN1:]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))# Use KMeans clustering on the training data\n",
        "n_clusters = len(all_classes)  # clustering into total number of classes\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(X_train_origin)\n",
        "\n",
        "# Predict cluster IDs for all data\n",
        "all_cluster_labels_train = kmeans.predict(X_train_origin)\n",
        "all_cluster_labels_test = kmeans.predict(X_test_origin)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "8W_mN6R4okoq",
        "outputId": "a26d034c-fffc-4eac-bf52-1e74e7e15b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1a06e00d38f7>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msvm_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_data_labels_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0my_test_pred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0my_test_labeled_Predicted2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestN1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0munknown_labels_Predicted2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestN1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0msvm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLIBSVM_IMPL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         return libsvm.predict(\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use KMeans clustering on the training data\n",
        "n_clusters = len(all_classes)  # clustering into total number of classes\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(X_train_origin)\n",
        "\n",
        "# Predict cluster IDs for all data\n",
        "all_cluster_labels_train = kmeans.predict(X_train_origin)\n",
        "all_cluster_labels_test = kmeans.predict(X_test_origin)\n",
        "\n",
        "# A mapping between cluster IDs and known classes\n",
        "# This mapping needs to be determined based on domain knowledge or some logic\n",
        "# Here's a dummy mapping as an example:\n",
        "col_ind = [0, 1, 2]  # map cluster 0 to known_classes[0], cluster 1 to known_classes[1], etc.\n",
        "\n",
        "rest_clusters = list(range(len(all_classes)))\n",
        "for i in range(len(col_ind)):\n",
        "    rest_clusters.remove(col_ind[i])\n",
        "print(rest_clusters)\n",
        "\n",
        "X_test_subset = X_test_origin[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "y_test_subset = y_test[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "\n",
        "# Combine training data with subset of test data from the 'rest' clusters\n",
        "all_data_combined = np.vstack([X_train_origin, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "# Predict using the trained SVM\n",
        "y_test_pred2 = svm_classifier.predict(X_test_origin)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:testN1]\n",
        "unknown_labels_Predicted2 = y_test_pred2[testN1:]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBewjXzVokrK",
        "outputId": "df9354f2-37f3-47dc-c706-b3d810e39a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "1358\n",
            "3463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# Evaluation of the methods\n",
        "# metrics from https://www.geeksforgeeks.org/clustering-performance-evaluation-in-scikit-learn/\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score, rand_score, fowlkes_mallows_score, silhouette_score, davies_bouldin_score, mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Results of the 1. method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\" \")\n",
        "print(\"Results of the 2. method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted2))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Ns1nGdokuM",
        "outputId": "2c55f469-4fd7-4a79-b0e6-858e5b6d7b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of the 1. method\n",
            "accuracy:  0.0625920471281296\n",
            "rand index:  0.4671171831691732\n",
            "adjusted rand index:  0.00022614368110274738\n",
            "fowlkes mallows :  0.3588729668439119\n",
            " \n",
            "Results of the 2. method\n",
            "accuracy:  0.46244477172312226\n",
            "rand index:  0.8470791246507396\n",
            "adjusted rand index:  0.6256625341388995\n",
            "fowlkes mallows :  0.7393483181624583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN + Kmeans"
      ],
      "metadata": {
        "id": "RrlDTZLo9KCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. method\n",
        "# Use an extended classifier, where the rest of clusters added as dummy classes\n",
        "# a. Training a classifier with:\n",
        "#    Train set with ground true labels + Subset of Test set (belonging to rest clusters)\n",
        "# b. Test phase of the classifier, where inference is the same as in 1. method\n",
        "from sklearn.metrics import adjusted_rand_score, rand_score, fowlkes_mallows_score, silhouette_score, davies_bouldin_score, mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "rest_clusters = list(range(len(all_classes)))\n",
        "for i in range(len(col_ind)):\n",
        "    rest_clusters.remove(col_ind[i])\n",
        "print(rest_clusters)\n",
        "\n",
        "X_test_subset = X_test[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "y_test_subset = y_test[np.isin(all_cluster_labels_test, rest_clusters)]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the Neural Network (MLP) using combined data and labels\n",
        "nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)\n",
        "nn_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = nn_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:testN1]\n",
        "unknown_labels_Predicted2 = y_test_pred2[testN1:]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "# (Remaining unchanged part)\n",
        "# Use KMeans clustering on the training data\n",
        "n_clusters = len(all_classes)  # clustering into total number of classes\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(X_train_origin)\n",
        "\n",
        "# Predict cluster IDs for all data\n",
        "all_cluster_labels_train = kmeans.predict(X_train_origin)\n",
        "all_cluster_labels_test = kmeans.predict(X_test_origin)\n",
        "\n",
        "\n",
        "print(\"Results of the method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted2))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted2))\n",
        "\n"
      ],
      "metadata": {
        "id": "xrMUSgZSgsan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89eab962-74b1-436c-83b7-fd5c16e3835c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1018\n",
            "5161\n",
            "Results of the method\n",
            "accuracy:  0.8261296660117878\n",
            "rand index:  0.7657880586209331\n",
            "adjusted rand index:  0.343847847548293\n",
            "fowlkes mallows :  0.5007345005796782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectral Clustering / Spectral Clustering + SVM"
      ],
      "metadata": {
        "id": "T_ibcpt0DMNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    print(data.keys())  # Add this line\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "# https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_classes = [2, 4, 5, 8, 9, 10, 13, 14]\n",
        "unknown_classes = [1, 3, 6, 7, 11, 12, 15, 16]\n",
        " # we need all classes without 0 class (because 0 is not class label)\n",
        "all_classes = np.concatenate((known_classes,unknown_classes), axis=0)\n",
        "\n",
        "all_data = flattened_data[np.isin(flattened_gt, all_classes)]\n",
        "all_labels = flattened_gt[np.isin(flattened_gt, all_classes)]\n",
        "\n",
        "known_data = all_data[np.isin(all_labels, known_classes)]\n",
        "known_labels = all_labels[np.isin(all_labels, known_classes)]\n",
        "unknown_data = all_data[np.isin(all_labels, unknown_classes)]\n",
        "unknown_labels = all_labels[np.isin(all_labels, unknown_classes)]\n",
        "\n",
        "# PCA: before OR after the splitting?\n",
        "# In this case: after the splitting\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled,unknown_data), axis=0)\n",
        "y_test = np.concatenate((y_test_labeled,unknown_labels), axis=0)\n",
        "\n",
        "trainN=len(y_train)\n",
        "testN1=len(y_test_labeled)\n",
        "testN2=len(unknown_labels)\n",
        "\n",
        "print(min_value)\n",
        "print(max_value)\n",
        "unique, counts = np.unique(flattened_gt, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)\n",
        "print(\"   \")\n",
        "print(data.shape)\n",
        "print(data.shape[0],\" x \", data.shape[1], \" = \", data.shape[0]*data.shape[1])\n",
        "print(known_data.shape)\n",
        "print(known_labels.shape)\n",
        "print(\"  \")\n",
        "print(X_train_origin.shape)\n",
        "print(X_test_origin.shape)\n",
        "print(trainN)\n",
        "print(testN1)\n",
        "print(testN2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AQiSSzxDY3S",
        "outputId": "7c2d7bed-87db-40a0-c150-8f9b45954334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
            "955\n",
            "9604\n",
            "[[    0 10776]\n",
            " [    1    46]\n",
            " [    2  1428]\n",
            " [    3   830]\n",
            " [    4   237]\n",
            " [    5   483]\n",
            " [    6   730]\n",
            " [    7    28]\n",
            " [    8   478]\n",
            " [    9    20]\n",
            " [   10   972]\n",
            " [   11  2455]\n",
            " [   12   593]\n",
            " [   13   205]\n",
            " [   14  1265]\n",
            " [   15   386]\n",
            " [   16    93]]\n",
            "   \n",
            "(145, 145, 200)\n",
            "145  x  145  =  21025\n",
            "(5088, 200)\n",
            "(5088,)\n",
            "  \n",
            "(4070, 200)\n",
            "(6179, 200)\n",
            "4070\n",
            "1018\n",
            "5161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=30)  # are the results worse without PCA?\n",
        "pca.fit(X_train_origin)\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)  # Transform all data using PCA\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbdv3UfeDY6x",
        "outputId": "4e1cb7d2-6fbe-425f-efc7-26a377e3b053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4070, 30)\n",
            "(6179, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spectral Clustering with 16 clusters (adjust number of clusters if needed)\n",
        "clustering = SpectralClustering(n_clusters=16, assign_labels='kmeans', random_state=42)\n",
        "# clustering = KMeans(n_clusters=6, random_state=42)\n",
        "all_data_pca = np.vstack([X_train, X_test])\n",
        "all_cluster_labels = clustering.fit_predict(all_data_pca)\n",
        "\n",
        "unique, counts = np.unique(all_cluster_labels, return_counts=True)\n",
        "print(unique, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egt7K9ZADY-0",
        "outputId": "db1896ea-f59b-4f4c-df21-26cc99ab8f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5] [2103 5024  891  595 1145  491]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign all_cluster_labels with class_labels in X_train\n",
        "import statistics\n",
        "from statistics import mode\n",
        "\n",
        "all_cluster_labels_train = all_cluster_labels[:trainN]\n",
        "all_cluster_labels_test = all_cluster_labels[trainN:]\n",
        "\n",
        "all_cluster_labels_a = all_cluster_labels_train[np.isin(y_train, known_classes[0])]\n",
        "all_cluster_labels_b = all_cluster_labels_train[np.isin(y_train, known_classes[1])]\n",
        "all_cluster_labels_c = all_cluster_labels_train[np.isin(y_train, known_classes[2])]\n",
        "print(len(all_cluster_labels_a))\n",
        "print(len(all_cluster_labels_b))\n",
        "print(len(all_cluster_labels_c))\n",
        "\n",
        "# print(statistics.mode(all_cluster_labels_a))\n",
        "# print(statistics.mode(all_cluster_labels_b))\n",
        "# print(statistics.mode(all_cluster_labels_c))\n",
        "\n",
        "# Statistics of Class1, Class10, Class11 with different clusters:\n",
        "# print(np.unique(all_cluster_labels_a, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_b, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_c, return_counts=True))\n",
        "\n",
        "AssignMatrix=np.zeros((3, 6))\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)\n",
        "print(AssignMatrix)\n",
        "\n",
        "# Instead of counts the relative ratio is better:\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)/len(all_cluster_labels_a)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)/len(all_cluster_labels_b)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)/len(all_cluster_labels_c)\n",
        "print(AssignMatrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g12C8RRPDZBj",
        "outputId": "bcee65d1-b2a7-41e7-b565-0dd7069ca95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1152\n",
            "185\n",
            "385\n",
            "[[ 17. 727. 345.   0.   0.  63.]\n",
            " [ 51.  45.  71.   0.   0.  18.]\n",
            " [122.   3.   0. 172.  85.   3.]]\n",
            "[[0.01475694 0.63107639 0.29947917 0.         0.         0.0546875 ]\n",
            " [0.27567568 0.24324324 0.38378378 0.         0.         0.0972973 ]\n",
            " [0.31688312 0.00779221 0.         0.44675325 0.22077922 0.00779221]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hungarian method for best (maximized) assignment\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "row_ind, col_ind = linear_sum_assignment(AssignMatrix, maximize='true')\n",
        "print(col_ind)\n",
        "\n",
        "# col_ind[0] is for class1, col_ind[1] is for class10, col_ind[2] is for class11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlYtekT_Dlcd",
        "outputId": "77562ff9-a5a0-4b37-c42d-2b36e34e1625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Method 1\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Use Spectral Clustering on all data\n",
        "n_clusters = len(all_classes)\n",
        "clustering = SpectralClustering(n_clusters=n_clusters, assign_labels='kmeans', random_state=42)\n",
        "\n",
        "all_cluster_labels = clustering.fit_predict(all_data)  # directly fit and predict on all_data\n",
        "\n",
        "col_ind = [0, 1, 2]\n",
        "for i in range(len(all_cluster_labels)):\n",
        "    for j in range(len(col_ind)):\n",
        "        if all_cluster_labels[i] == col_ind[j]:\n",
        "            all_cluster_labels[i] = known_classes[j]\n",
        "\n",
        "y_test_labeled_Predicted = all_cluster_labels[trainN:trainN+testN1]\n",
        "unknown_labels_Predicted = all_cluster_labels[trainN+testN1:trainN+testN1+testN2]\n",
        "\n",
        "print(len(y_test_labeled_Predicted))\n",
        "print(len(unknown_labels_Predicted))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM--sRvktCMI",
        "outputId": "a6b16ef6-b7ee-4413-b394-9a6c815f30a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1018\n",
            "5161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Use Spectral Clustering on all data\n",
        "n_clusters = len(all_classes)\n",
        "clustering = SpectralClustering(n_clusters=n_clusters, assign_labels='kmeans', random_state=42)\n",
        "\n",
        "all_cluster_labels = clustering.fit_predict(all_data)  # directly fit and predict on all_data\n",
        "\n",
        "# A mapping between cluster IDs and known classes\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(set(all_cluster_labels) - set(col_ind))\n",
        "\n",
        "X_test_subset = X_test_origin[np.isin(all_cluster_labels[trainN:], rest_clusters)]\n",
        "y_test_subset = y_test[np.isin(all_cluster_labels[trainN:], rest_clusters)]\n",
        "\n",
        "# Combine training data with subset of test data from the 'rest' clusters\n",
        "all_data_combined = np.vstack([X_train_origin, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "# Predict using the trained SVM\n",
        "y_test_pred2 = svm_classifier.predict(X_test_origin)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:testN1]\n",
        "unknown_labels_Predicted2 = y_test_pred2[testN1:]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01V96sbIsys2",
        "outputId": "25623cb3-76f6-4aa8-853a-4107e7c08543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1018\n",
            "5161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# Evaluation of the methods\n",
        "# metrics from https://www.geeksforgeeks.org/clustering-performance-evaluation-in-scikit-learn/\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score, rand_score, fowlkes_mallows_score, silhouette_score, davies_bouldin_score, mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Results of the 1. method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted))\n",
        "print(\" \")\n",
        "print(\"Results of the 2. method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted2))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll7RdlcNEYrZ",
        "outputId": "21145035-6082-4ed0-dbe1-9db50cd4372e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of the 1. method\n",
            "accuracy:  0.06974459724950884\n",
            "rand index:  0.6432351911849306\n",
            "adjusted rand index:  0.009069313995271779\n",
            "fowlkes mallows :  0.2265083771343133\n",
            " \n",
            "Results of the 2. method\n",
            "accuracy:  0.5599214145383105\n",
            "rand index:  0.7222341382671768\n",
            "adjusted rand index:  0.41177491173299174\n",
            "fowlkes mallows :  0.6289327029563248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN + Spectral Clustering"
      ],
      "metadata": {
        "id": "LdMixqUVQHc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    print(data.keys())  # Add this line\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "# https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_classes = [1, 2, 3, 4, 9, 10, 11, 12, 13]\n",
        "unknown_classes = [5, 6, 7, 8, 14, 15, 16]\n",
        " # we need all classes without 0 class (because 0 is not class label)\n",
        "all_classes = np.concatenate((known_classes,unknown_classes), axis=0)\n",
        "\n",
        "all_data = flattened_data[np.isin(flattened_gt, all_classes)]\n",
        "all_labels = flattened_gt[np.isin(flattened_gt, all_classes)]\n",
        "\n",
        "known_data = all_data[np.isin(all_labels, known_classes)]\n",
        "known_labels = all_labels[np.isin(all_labels, known_classes)]\n",
        "unknown_data = all_data[np.isin(all_labels, unknown_classes)]\n",
        "unknown_labels = all_labels[np.isin(all_labels, unknown_classes)]\n",
        "\n",
        "# PCA: before OR after the splitting?\n",
        "# In this case: after the splitting\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled,unknown_data), axis=0)\n",
        "y_test = np.concatenate((y_test_labeled,unknown_labels), axis=0)\n",
        "\n",
        "trainN=len(y_train)\n",
        "testN1=len(y_test_labeled)\n",
        "testN2=len(unknown_labels)\n",
        "\n",
        "print(min_value)\n",
        "print(max_value)\n",
        "unique, counts = np.unique(flattened_gt, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)\n",
        "print(\"   \")\n",
        "print(data.shape)\n",
        "print(data.shape[0],\" x \", data.shape[1], \" = \", data.shape[0]*data.shape[1])\n",
        "print(known_data.shape)\n",
        "print(known_labels.shape)\n",
        "print(\"  \")\n",
        "print(X_train_origin.shape)\n",
        "print(X_test_origin.shape)\n",
        "print(trainN)\n",
        "print(testN1)\n",
        "print(testN2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmgt4J4xQb-e",
        "outputId": "3f5c6682-5aaa-49da-cdf9-d4fe6263fd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
            "955\n",
            "9604\n",
            "[[    0 10776]\n",
            " [    1    46]\n",
            " [    2  1428]\n",
            " [    3   830]\n",
            " [    4   237]\n",
            " [    5   483]\n",
            " [    6   730]\n",
            " [    7    28]\n",
            " [    8   478]\n",
            " [    9    20]\n",
            " [   10   972]\n",
            " [   11  2455]\n",
            " [   12   593]\n",
            " [   13   205]\n",
            " [   14  1265]\n",
            " [   15   386]\n",
            " [   16    93]]\n",
            "   \n",
            "(145, 145, 200)\n",
            "145  x  145  =  21025\n",
            "(6786, 200)\n",
            "(6786,)\n",
            "  \n",
            "(5428, 200)\n",
            "(4821, 200)\n",
            "5428\n",
            "1358\n",
            "3463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=30)  # are the results worse without PCA?\n",
        "pca.fit(X_train_origin)\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)  # Transform all data using PCA\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgXuZPyaQcA_",
        "outputId": "58056215-8646-49af-9b66-ea8da186f114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5428, 30)\n",
            "(4821, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spectral Clustering with 16 clusters (adjust number of clusters if needed)\n",
        "clustering = SpectralClustering(n_clusters=16, assign_labels='kmeans', random_state=42)\n",
        "# clustering = KMeans(n_clusters=6, random_state=42)\n",
        "all_data_pca = np.vstack([X_train, X_test])\n",
        "all_cluster_labels = clustering.fit_predict(all_data_pca)\n",
        "\n",
        "unique, counts = np.unique(all_cluster_labels, return_counts=True)\n",
        "print(unique, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIniANYVQcD9",
        "outputId": "5c0b9b44-9248-4a35-da3e-e16c6a9b42c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] [ 490 2853    2  678   76  831  558  111    5 2351  373   67  640  479\n",
            "  185  550]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign all_cluster_labels with class_labels in X_train\n",
        "import statistics\n",
        "from statistics import mode\n",
        "\n",
        "all_cluster_labels_train = all_cluster_labels[:trainN]\n",
        "all_cluster_labels_test = all_cluster_labels[trainN:]\n",
        "\n",
        "all_cluster_labels_a = all_cluster_labels_train[np.isin(y_train, known_classes[0])]\n",
        "all_cluster_labels_b = all_cluster_labels_train[np.isin(y_train, known_classes[1])]\n",
        "all_cluster_labels_c = all_cluster_labels_train[np.isin(y_train, known_classes[2])]\n",
        "print(len(all_cluster_labels_a))\n",
        "print(len(all_cluster_labels_b))\n",
        "print(len(all_cluster_labels_c))\n",
        "\n",
        "# print(statistics.mode(all_cluster_labels_a))\n",
        "# print(statistics.mode(all_cluster_labels_b))\n",
        "# print(statistics.mode(all_cluster_labels_c))\n",
        "\n",
        "# Statistics of Class1, Class10, Class11 with different clusters:\n",
        "# print(np.unique(all_cluster_labels_a, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_b, return_counts=True))\n",
        "# print(np.unique(all_cluster_labels_c, return_counts=True))\n",
        "\n",
        "AssignMatrix=np.zeros((3, 6))\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)\n",
        "print(AssignMatrix)\n",
        "\n",
        "# Instead of counts the relative ratio is better:\n",
        "for i in range(6):\n",
        "  AssignMatrix[0][i]=list(all_cluster_labels_a).count(i)/len(all_cluster_labels_a)\n",
        "for i in range(6):\n",
        "  AssignMatrix[1][i]=list(all_cluster_labels_b).count(i)/len(all_cluster_labels_b)\n",
        "for i in range(6):\n",
        "  AssignMatrix[2][i]=list(all_cluster_labels_c).count(i)/len(all_cluster_labels_c)\n",
        "print(AssignMatrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njl-3aOYQcHA",
        "outputId": "ec5b443f-b34c-4f03-be4c-e5123d97154d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "1158\n",
            "663\n",
            "[[  0.   0.   0.   0.   0.   1.]\n",
            " [  3. 336.   0.   0.   0.   4.]\n",
            " [  0. 334.   0.   0.   0.   0.]]\n",
            "[[0.         0.         0.         0.         0.         0.02564103]\n",
            " [0.00259067 0.29015544 0.         0.         0.         0.00345423]\n",
            " [0.         0.50377074 0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.neural_network import MLPClassifier  # Importing MLPClassifier\n",
        "\n",
        "# Use Spectral Clustering on all data\n",
        "n_clusters = len(all_classes)\n",
        "clustering = SpectralClustering(n_clusters=n_clusters, assign_labels='kmeans', random_state=42)\n",
        "\n",
        "all_cluster_labels = clustering.fit_predict(all_data)  # directly fit and predict on all_data\n",
        "\n",
        "# A mapping between cluster IDs and known classes\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(set(all_cluster_labels) - set(col_ind))\n",
        "\n",
        "X_test_subset = X_test_origin[np.isin(all_cluster_labels[trainN:], rest_clusters)]\n",
        "y_test_subset = y_test[np.isin(all_cluster_labels[trainN:], rest_clusters)]\n",
        "\n",
        "# Combine training data with subset of test data from the 'rest' clusters\n",
        "all_data_combined = np.vstack([X_train_origin, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the NN using combined data and labels\n",
        "nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)  # Create MLP classifier\n",
        "nn_classifier.fit(all_data_combined, all_data_labels_combined)  # Train the classifier\n",
        "\n",
        "# Predict using the trained NN\n",
        "y_test_pred2 = nn_classifier.predict(X_test_origin)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:testN1]\n",
        "unknown_labels_Predicted2 = y_test_pred2[testN1:]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l3tPIBFQGbz",
        "outputId": "db81593e-16ec-41a5-ee73-aef84d1c979e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1358\n",
            "3463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Results of the NN Spectral method\")\n",
        "print(\"accuracy: \", accuracy_score(y_test_labeled, y_test_labeled_Predicted2))\n",
        "print(\"rand index: \", rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"adjusted rand index: \", adjusted_rand_score(unknown_labels, unknown_labels_Predicted2))\n",
        "print(\"fowlkes mallows : \", fowlkes_mallows_score(unknown_labels, unknown_labels_Predicted2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDyBMYf9RU58",
        "outputId": "20adf7ad-26f2-485e-e54d-c354f08c2c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of the NN Spectral method\n",
            "accuracy:  0.508100147275405\n",
            "rand index:  0.8657433797545832\n",
            "adjusted rand index:  0.6468001372349084\n",
            "fowlkes mallows :  0.739119693631832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semi Kmeans"
      ],
      "metadata": {
        "id": "IHj2V_YG5Vm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split based on sample size"
      ],
      "metadata": {
        "id": "vdEDpAXW5Ziv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sgvaze/generalized-category-discovery.git"
      ],
      "metadata": {
        "id": "cB6PHCBs7nSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2733f300-8316-49c8-e61c-2204c6b83e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'generalized-category-discovery'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 76 (delta 13), reused 53 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (76/76), 3.70 MiB | 8.97 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv generalized-category-discovery gcd\n",
        "\n",
        "# After executing this command you will get another error, to solve the new error do the following:\n",
        "# 1- open \"faster_mix_k_means_pytorch\" file in the notebook\n",
        "# 2- change the 4th line as the following:\n",
        "# old: from project_utils.cluster_utils import cluster_acc\n",
        "# new: from gcd.project_utils.cluster_utils import cluster_acc\n",
        "# 3- save the file"
      ],
      "metadata": {
        "id": "ZMsX-5XNRZSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "iZJGeIfPRbmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15]\n",
        "unknown_classes = [1, 7, 9, 16]\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Set k to the number of known classes\n",
        "k_clusters = len(known_classes)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=k_clusters, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKx8fqo9TEfB",
        "outputId": "6e7fdf19-43ac-48d5-bed3-8678528282f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8049, 30) (2200, 30) (8049,) (2013,)\n",
            "Training accuracy: 51.42%\n",
            "Accuracy: 51.42%\n",
            "Rand Index: 0.2102\n",
            "Adjusted Rand Index: 0.3062\n",
            "Fowlkes-Mallows Index: 0.4670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15]\n",
        "unknown_classes = [1, 7, 9, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49yDOIZzto4K",
        "outputId": "d29133fb-8ea4-4c30-bc15-8e1b06c71810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8049, 30) (2200, 30) (8049,) (2013,)\n",
            "Training accuracy: 52.07%\n",
            "Accuracy: 52.07%\n",
            "Rand Index: 0.1801\n",
            "Adjusted Rand Index: 0.3215\n",
            "Fowlkes-Mallows Index: 0.4531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on feature type"
      ],
      "metadata": {
        "id": "XyMAakNE5pAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [5, 6, 14]\n",
        "unknown_classes = [1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 15, 16]\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Set k to the number of known classes\n",
        "k_clusters = len(known_classes)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=k_clusters, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n85HLhQV5Ix-",
        "outputId": "d7e10e7c-edff-4eb6-a92b-6148e8bcbbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1982, 30) (496, 30) (1982,) (496,)\n",
            "(7771, 30) (2478, 30)\n",
            "(1982, 30) (8267, 30) (1982,) (496,)\n",
            "Training accuracy: 74.17%\n",
            "Accuracy: 74.17%\n",
            "Rand Index: 0.4826\n",
            "Adjusted Rand Index: 0.4468\n",
            "Fowlkes-Mallows Index: 0.7175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [5, 6, 14]\n",
        "unknown_classes = [1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPQdhvKZtapX",
        "outputId": "d6ff38e4-2b38-4c00-81ef-aca32723a35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1982, 30) (8267, 30) (1982,) (496,)\n",
            "Training accuracy: 86.73%\n",
            "Accuracy: 86.73%\n",
            "Rand Index: 0.3908\n",
            "Adjusted Rand Index: 0.6732\n",
            "Fowlkes-Mallows Index: 0.8009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random division 1"
      ],
      "metadata": {
        "id": "wY69Ok_X5kgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 4, 5, 8, 9, 10, 13, 14]\n",
        "unknown_classes = [1, 3, 6, 7, 11, 12, 15, 16]\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Set k to the number of known classes\n",
        "k_clusters = len(known_classes)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=k_clusters, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRZyWd0D40GN",
        "outputId": "62110221-0c26-4971-afa1-537d12b6b21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1982, 30) (496, 30) (1982,) (496,)\n",
            "(7771, 30) (2478, 30)\n",
            "(4070, 30) (6179, 30) (4070,) (1018,)\n",
            "Training accuracy: 68.16%\n",
            "Accuracy: 68.16%\n",
            "Rand Index: 0.2248\n",
            "Adjusted Rand Index: 0.5117\n",
            "Fowlkes-Mallows Index: 0.6240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 4, 5, 8, 9, 10, 13, 14]\n",
        "unknown_classes = [1, 3, 6, 7, 11, 12, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTOYz-DMtIqw",
        "outputId": "8fed1d67-dae2-4d20-d5ff-dfa971af282b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4070, 30) (6179, 30) (4070,) (1018,)\n",
            "Training accuracy: 70.96%\n",
            "Accuracy: 70.96%\n",
            "Rand Index: 0.2340\n",
            "Adjusted Rand Index: 0.5463\n",
            "Fowlkes-Mallows Index: 0.6573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random division2"
      ],
      "metadata": {
        "id": "M09tZwyu5tUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 5, 6, 8, 11, 13, 15]\n",
        "unknown_classes = [3, 4, 7, 9, 10, 12, 14, 16]\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Set k to the number of known classes\n",
        "k_clusters = len(known_classes)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=k_clusters, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9-GLRfz5SdN",
        "outputId": "e2b8775a-44d8-4d5d-8be0-96a611050e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4968, 30) (5281, 30) (4968,) (1243,)\n",
            "Training accuracy: 68.08%\n",
            "Accuracy: 68.08%\n",
            "Rand Index: 0.2989\n",
            "Adjusted Rand Index: 0.5092\n",
            "Fowlkes-Mallows Index: 0.6631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 5, 6, 8, 11, 13, 15]\n",
        "unknown_classes = [3, 4, 7, 9, 10, 12, 14, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk35cS1ts210",
        "outputId": "ddcbbad8-c543-467f-83b1-14d5f94ab336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4968, 30) (5281, 30) (4968,) (1243,)\n",
            "Training accuracy: 70.09%\n",
            "Accuracy: 70.09%\n",
            "Rand Index: 0.2657\n",
            "Adjusted Rand Index: 0.5154\n",
            "Fowlkes-Mallows Index: 0.6459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crop-based divisions:"
      ],
      "metadata": {
        "id": "fswK5eaa6OAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 3, 4, 9, 10, 11, 12, 13]\n",
        "unknown_classes = [5, 6, 7, 8, 14, 15, 16]\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Set k to the number of known classes\n",
        "k_clusters = len(known_classes)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=k_clusters, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1srF2BhM6PCG",
        "outputId": "dc74b8f6-8a4e-418e-f267-f637bceeaabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5428, 30) (4821, 30) (5428,) (1358,)\n",
            "Training accuracy: 44.93%\n",
            "Accuracy: 44.93%\n",
            "Rand Index: 0.3149\n",
            "Adjusted Rand Index: 0.1244\n",
            "Fowlkes-Mallows Index: 0.3931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import adjusted_rand_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import comb\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 3, 4, 9, 10, 11, 12, 13]\n",
        "unknown_classes = [5, 6, 7, 8, 14, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test_labeled.shape)\n",
        "\n",
        "y_train_true = y_train.copy()\n",
        "y_test_true = y_test_labeled.copy()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Map cluster labels back to original labels\n",
        "cluster_labels = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "\n",
        "y_train_pred = np.empty_like(y_train)\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    mask = (cluster_labels == cluster)\n",
        "    if mask.sum() > 0:\n",
        "        common_label_mode = mode(y_train_true[mask]).mode\n",
        "        common_label = common_label_mode[0] if np.ndim(common_label_mode) > 0 else common_label_mode\n",
        "        y_train_pred[mask] = common_label\n",
        "\n",
        "# Calculate and print accuracy\n",
        "train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Combine the true labels and predicted labels\n",
        "y_true_combined = y_train_true\n",
        "y_pred_combined = y_train_pred\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "# Calculate metrics\n",
        "ari = adjusted_rand_score(y_true_combined, y_pred_combined)\n",
        "fmi = fowlkes_mallows_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "def rand_index_score(y_true, y_pred):\n",
        "    # Create a confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate values\n",
        "    a = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=1)])\n",
        "    b = 0.5 * np.sum([comb(k, 2) for k in cm.sum(axis=0)])\n",
        "    c = 0.5 * np.sum([comb(k, 2) for k in np.ravel(cm)])\n",
        "    n = comb(len(y_true), 2)\n",
        "\n",
        "    # Compute the Rand Index\n",
        "    ri = (a + b) / n\n",
        "    return ri\n",
        "\n",
        "# Calculate Rand Index\n",
        "ri = rand_index_score(y_true_combined, y_pred_combined)\n",
        "\n",
        "print(f\"Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "print(f\"Rand Index: {ri:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"Fowlkes-Mallows Index: {fmi:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA6PiM08rulE",
        "outputId": "620fb980-dffe-4862-b4af-e0912df6a986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5428, 30) (4821, 30) (5428,) (1358,)\n",
            "Training accuracy: 46.55%\n",
            "Accuracy: 46.55%\n",
            "Rand Index: 0.3107\n",
            "Adjusted Rand Index: 0.1364\n",
            "Fowlkes-Mallows Index: 0.3990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `SEMI Kmeans`+ SVM"
      ],
      "metadata": {
        "id": "s-efD9Yzgvow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crop-based divisions:"
      ],
      "metadata": {
        "id": "dLx3PU5JZkQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 3, 4, 9, 10, 11, 12, 13]\n",
        "unknown_classes = [5, 6, 7, 8, 14, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "# Assuming the closest centroids can be our cluster prediction for test data.\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "# Create a boolean index based on the test labels\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "\n",
        "# Use the boolean index to subset the test data\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "\n",
        "# Now, since X_test_subset might contain data both from X_test_labeled and unknown_data,\n",
        "# we should also subset the labels. To do this, we create a combined y_test which covers all data in X_test.\n",
        "# Here, we assign a label of -1 for unknown_data, just to keep the shape consistent.\n",
        "# NOTE: These -1 labels are not used for training the SVM, they are just placeholders.\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "\n",
        "# Now, we can use mask_test_subset to subset this combined y_test.\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = svm_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "# Real tags include real tags for known categories and real tags for unknown categories\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "# Calculate assessment indicators\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    \"\"\"Compute the Rand Index.\"\"\"\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    \"\"\"The number of combinations of n things taken 2 at a time.\"\"\"\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)  # Fixed this line\n",
        "print(\"Fowlkes Mallows Score:\", fms)  # Fixed this line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vi-e8uzToIg",
        "outputId": "1b112e46-40b8-4144-b957-5061a85e65b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1358\n",
            "3463\n",
            "Rand Index: 0.2062372154765192\n",
            "Accuracy: 0.6539027982326951\n",
            "Adjusted Rand Index: 0.2062372154765192\n",
            "Fowlkes Mallows Score: 0.4727867332573832\n",
            "Fowlkes Mallows Score: 0.4918697609785762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random division 2:"
      ],
      "metadata": {
        "id": "WVkIFAafZdYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 5, 6, 8, 11, 13, 15]\n",
        "unknown_classes = [3, 4, 7, 9, 10, 12, 14, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "# Assuming the closest centroids can be our cluster prediction for test data.\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "# Create a boolean index based on the test labels\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "\n",
        "# Use the boolean index to subset the test data\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "\n",
        "# Now, since X_test_subset might contain data both from X_test_labeled and unknown_data,\n",
        "# we should also subset the labels. To do this, we create a combined y_test which covers all data in X_test.\n",
        "# Here, we assign a label of -1 for unknown_data, just to keep the shape consistent.\n",
        "# NOTE: These -1 labels are not used for training the SVM, they are just placeholders.\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "\n",
        "# Now, we can use mask_test_subset to subset this combined y_test.\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = svm_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "# Real tags include real tags for known categories and real tags for unknown categories\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "# Calculate assessment indicators\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    \"\"\"Compute the Rand Index.\"\"\"\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    \"\"\"The number of combinations of n things taken 2 at a time.\"\"\"\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)  # Fixed this line\n",
        "print(\"Fowlkes Mallows Score:\", fms)  # Fixed this line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2UE5o-bZUZ1",
        "outputId": "ed1a60bf-e528-4960-8f44-bc07e482ece6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1243\n",
            "4038\n",
            "Rand Index: 0.1343409205399869\n",
            "Accuracy: 0.7691069991954947\n",
            "Adjusted Rand Index: 0.13434092053998692\n",
            "Fowlkes Mallows Score: 0.3468427092572668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on feature type:"
      ],
      "metadata": {
        "id": "a6v0beFFZvdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [5, 6, 14]\n",
        "unknown_classes = [1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "# Assuming the closest centroids can be our cluster prediction for test data.\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "# Create a boolean index based on the test labels\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "\n",
        "# Use the boolean index to subset the test data\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "\n",
        "# Now, since X_test_subset might contain data both from X_test_labeled and unknown_data,\n",
        "# we should also subset the labels. To do this, we create a combined y_test which covers all data in X_test.\n",
        "# Here, we assign a label of -1 for unknown_data, just to keep the shape consistent.\n",
        "# NOTE: These -1 labels are not used for training the SVM, they are just placeholders.\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "\n",
        "# Now, we can use mask_test_subset to subset this combined y_test.\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = svm_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "# Real tags include real tags for known categories and real tags for unknown categories\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "# Calculate assessment indicators\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    \"\"\"Compute the Rand Index.\"\"\"\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    \"\"\"The number of combinations of n things taken 2 at a time.\"\"\"\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)  # Fixed this line\n",
        "print(\"Fowlkes Mallows Score:\", fms)  # Fixed this line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuMfZcevZvls",
        "outputId": "86a01dba-6b25-422f-e0db-4d128c33e732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "496\n",
            "7771\n",
            "Rand Index: 0.06384410343570252\n",
            "Accuracy: 0.9415322580645161\n",
            "Adjusted Rand Index: 0.06384410343570252\n",
            "Fowlkes Mallows Score: 0.4247424236481274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on sample size"
      ],
      "metadata": {
        "id": "iRZaZNFZaKal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15]\n",
        "unknown_classes = [1, 7, 9, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "# Assuming the closest centroids can be our cluster prediction for test data.\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "# Create a boolean index based on the test labels\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "\n",
        "# Use the boolean index to subset the test data\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "\n",
        "# Now, since X_test_subset might contain data both from X_test_labeled and unknown_data,\n",
        "# we should also subset the labels. To do this, we create a combined y_test which covers all data in X_test.\n",
        "# Here, we assign a label of -1 for unknown_data, just to keep the shape consistent.\n",
        "# NOTE: These -1 labels are not used for training the SVM, they are just placeholders.\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "\n",
        "# Now, we can use mask_test_subset to subset this combined y_test.\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = svm_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "# Real tags include real tags for known categories and real tags for unknown categories\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "# Calculate assessment indicators\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    \"\"\"Compute the Rand Index.\"\"\"\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    \"\"\"The number of combinations of n things taken 2 at a time.\"\"\"\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)  # Fixed this line\n",
        "print(\"Fowlkes Mallows Score:\", fms)  # Fixed this line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR6TgSjjaLMK",
        "outputId": "c395cdb2-257f-43c3-a837-bab735c1e475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013\n",
            "187\n",
            "Rand Index: 0.5076934125151138\n",
            "Accuracy: 0.7401887729756582\n",
            "Adjusted Rand Index: 0.5076934125151138\n",
            "Fowlkes Mallows Score: 0.5736815122394838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random division 1:"
      ],
      "metadata": {
        "id": "c9Rl4ijzapD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 4, 5, 8, 9, 10, 13, 14]\n",
        "unknown_classes = [1, 3, 6, 7, 11, 12, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "# Assuming the closest centroids can be our cluster prediction for test data.\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "# Create a boolean index based on the test labels\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "\n",
        "# Use the boolean index to subset the test data\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "\n",
        "# Now, since X_test_subset might contain data both from X_test_labeled and unknown_data,\n",
        "# we should also subset the labels. To do this, we create a combined y_test which covers all data in X_test.\n",
        "# Here, we assign a label of -1 for unknown_data, just to keep the shape consistent.\n",
        "# NOTE: These -1 labels are not used for training the SVM, they are just placeholders.\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "\n",
        "# Now, we can use mask_test_subset to subset this combined y_test.\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the SVM using combined data and labels\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = svm_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "# Real tags include real tags for known categories and real tags for unknown categories\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "# Calculate assessment indicators\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    \"\"\"Compute the Rand Index.\"\"\"\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    \"\"\"The number of combinations of n things taken 2 at a time.\"\"\"\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)  # Fixed this line\n",
        "print(\"Fowlkes Mallows Score:\", fms)  # Fixed this line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJdp6W8YapkR",
        "outputId": "65c3fca2-3316-4bc6-a135-e7cb43de725e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1018\n",
            "5161\n",
            "Rand Index: 0.14035825564596224\n",
            "Accuracy: 0.6679764243614931\n",
            "Adjusted Rand Index: 0.14035825564596227\n",
            "Fowlkes Mallows Score: 0.4494561290738025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SemiSup Kmeans + Neaural Network"
      ],
      "metadata": {
        "id": "HpQWUTQXdyl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random division 1:"
      ],
      "metadata": {
        "id": "hfQ5VKRlegDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from sklearn.neural_network import MLPClassifier  # Importing the MLPClassifier\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 4, 5, 8, 9, 10, 13, 14]\n",
        "unknown_classes = [1, 3, 6, 7, 11, 12, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the Neural Network (MLP) using combined data and labels\n",
        "nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)\n",
        "nn_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = nn_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)\n",
        "print(\"Fowlkes Mallows Score:\", fms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8iyfT6Td5To",
        "outputId": "1c41e1b6-2973-44a5-d3f0-63d409e49955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1018\n",
            "5161\n",
            "Rand Index: 0.1360530151535562\n",
            "Accuracy: 0.7819253438113949\n",
            "Adjusted Rand Index: 0.13605301515355625\n",
            "Fowlkes Mallows Score: 0.44076660276571206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on sample size:"
      ],
      "metadata": {
        "id": "6qz0W0WFekt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from sklearn.neural_network import MLPClassifier  # Importing the MLPClassifier\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15]\n",
        "unknown_classes = [1, 7, 9, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the Neural Network (MLP) using combined data and labels\n",
        "nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)\n",
        "nn_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = nn_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)\n",
        "print(\"Fowlkes Mallows Score:\", fms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F05YW9N5etCQ",
        "outputId": "03f51d44-b247-4997-a0b9-49fcd03383c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013\n",
            "187\n",
            "Rand Index: 0.5644583827585434\n",
            "Accuracy: 0.7754595131644312\n",
            "Adjusted Rand Index: 0.5644583827585434\n",
            "Fowlkes Mallows Score: 0.6148338242155408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on feature type:"
      ],
      "metadata": {
        "id": "JD95EBWRfO-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from sklearn.neural_network import MLPClassifier  # Importing the MLPClassifier\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [5, 6, 14]\n",
        "unknown_classes = [1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the Neural Network (MLP) using combined data and labels\n",
        "nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)\n",
        "nn_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = nn_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)\n",
        "print(\"Fowlkes Mallows Score:\", fms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73G-hVLTfPIG",
        "outputId": "d6983eda-20cf-4dd5-a40f-053b18ed12e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "496\n",
            "7771\n",
            "Rand Index: 0.05473057147905564\n",
            "Accuracy: 0.9536290322580645\n",
            "Adjusted Rand Index: 0.05473057147905563\n",
            "Fowlkes Mallows Score: 0.4206174446049294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random division 2:"
      ],
      "metadata": {
        "id": "rVdcQnrJfjxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from sklearn.neural_network import MLPClassifier  # Importing the MLPClassifier\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 5, 6, 8, 11, 13, 15]\n",
        "unknown_classes = [3, 4, 7, 9, 10, 12, 14, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the Neural Network (MLP) using combined data and labels\n",
        "nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)\n",
        "nn_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = nn_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)\n",
        "print(\"Fowlkes Mallows Score:\", fms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKHIW5Jrfj8a",
        "outputId": "af664f4a-a458-4599-9a07-38d418e20dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1243\n",
            "4038\n",
            "Rand Index: 0.14043219864718193\n",
            "Accuracy: 0.7787610619469026\n",
            "Adjusted Rand Index: 0.1404321986471819\n",
            "Fowlkes Mallows Score: 0.34768231184960885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crop-based divisions:"
      ],
      "metadata": {
        "id": "FxV0MAYsfz8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import mode\n",
        "import torch\n",
        "from sklearn.neural_network import MLPClassifier  # Importing the MLPClassifier\n",
        "from gcd.methods.clustering.faster_mix_k_means_pytorch import K_Means as SemiSupKMeans\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score, fowlkes_mallows_score\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_corrected']\n",
        "\n",
        "def load_gt(file_path):\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "    return data['indian_pines_gt']\n",
        "\n",
        "data = load_data('/content/Indian_pines_corrected.mat')\n",
        "gt_data = load_gt('/content/Indian_pines_gt.mat')\n",
        "\n",
        "# Normalize data\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "normalized_data = (data - min_value) / (max_value - min_value)\n",
        "\n",
        "# Data Processing\n",
        "known_classes = [1, 2, 3, 4, 9, 10, 11, 12, 13]\n",
        "unknown_classes = [5, 6, 7, 8, 14, 15, 16]\n",
        "all_classes = known_classes + unknown_classes\n",
        "large_N = len(all_classes)\n",
        "\n",
        "flattened_data = normalized_data.reshape((-1, normalized_data.shape[-1]))\n",
        "flattened_gt = gt_data.flatten()\n",
        "\n",
        "known_data = flattened_data[np.isin(flattened_gt, known_classes)]\n",
        "known_labels = flattened_gt[np.isin(flattened_gt, known_classes)]\n",
        "unknown_data = flattened_data[np.isin(flattened_gt, unknown_classes)]\n",
        "\n",
        "X_train_origin, X_test_labeled, y_train, y_test_labeled = train_test_split(known_data, known_labels, test_size=0.2, random_state=42)\n",
        "X_test_origin = np.concatenate((X_test_labeled, unknown_data), axis=0)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(known_data)\n",
        "\n",
        "X_train = pca.transform(X_train_origin)\n",
        "X_test = pca.transform(X_test_origin)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_tensor = torch.from_numpy(X_train).to(device)\n",
        "y_train_true_tensor = torch.tensor(y_train).to(device)\n",
        "\n",
        "# Create and fit the semi-supervised K-Means model\n",
        "semi_sup_kmeans = SemiSupKMeans(k=large_N, init='k-means++', n_init=10, random_state=42, n_jobs=None, pairwise_batch_size=10)\n",
        "semi_sup_kmeans.fit(X_train_tensor)\n",
        "\n",
        "# Get cluster labels\n",
        "all_cluster_labels_train = semi_sup_kmeans.labels_.cpu().numpy()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
        "with torch.no_grad():\n",
        "    dist = ((X_test_tensor.unsqueeze(1) - semi_sup_kmeans.cluster_centers_)**2).sum(-1)\n",
        "    all_cluster_labels_test = dist.argmin(1).cpu().numpy()\n",
        "\n",
        "# The rest of method2 using the cluster IDs\n",
        "col_ind = [0, 1, 2]\n",
        "rest_clusters = list(range(large_N))\n",
        "for i in col_ind:\n",
        "    rest_clusters.remove(i)\n",
        "\n",
        "mask_test_subset = np.isin(all_cluster_labels_test, rest_clusters)\n",
        "X_test_subset = X_test[mask_test_subset]\n",
        "y_test_combined = np.concatenate([y_test_labeled, [-1 for _ in range(len(unknown_data))]])\n",
        "y_test_subset = y_test_combined[mask_test_subset]\n",
        "\n",
        "all_data_combined = np.vstack([X_train, X_test_subset])\n",
        "all_data_labels_combined = np.concatenate([y_train, y_test_subset])\n",
        "\n",
        "# Train the Neural Network (MLP) using combined data and labels\n",
        "nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)\n",
        "nn_classifier.fit(all_data_combined, all_data_labels_combined)\n",
        "\n",
        "y_test_pred2 = nn_classifier.predict(X_test)\n",
        "y_test_labeled_Predicted2 = y_test_pred2[:len(y_test_labeled)]\n",
        "unknown_labels_Predicted2 = y_test_pred2[len(y_test_labeled):]\n",
        "\n",
        "print(len(y_test_labeled_Predicted2))\n",
        "print(len(unknown_labels_Predicted2))\n",
        "\n",
        "y_test_all = np.concatenate([y_test_labeled, flattened_gt[np.isin(flattened_gt, unknown_classes)]])\n",
        "\n",
        "accuracy = accuracy_score(y_test_labeled, y_test_labeled_Predicted2)\n",
        "ari = adjusted_rand_score(y_test_all, y_test_pred2)\n",
        "fms = fowlkes_mallows_score(y_test_all, y_test_pred2)\n",
        "\n",
        "def rand_index(labels_true, labels_pred):\n",
        "    contingency = confusion_matrix(labels_true, labels_pred)\n",
        "    n = contingency.sum()\n",
        "    sum_comb = sum(sum(comb2(i) for i in row) for row in contingency)\n",
        "    sum_comb_rows = sum(comb2(r.sum()) for r in contingency)\n",
        "    sum_comb_cols = sum(comb2(c.sum()) for c in contingency.T)\n",
        "    expected_index = sum_comb_rows * sum_comb_cols / comb2(n)\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    return (sum_comb - expected_index) / (max_index - expected_index)\n",
        "\n",
        "def comb2(n):\n",
        "    return comb(n, 2)\n",
        "\n",
        "ri = rand_index(y_test_all, y_test_pred2)\n",
        "print(\"Rand Index:\", ri)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Adjusted Rand Index:\", ari)\n",
        "print(\"Fowlkes Mallows Score:\", fms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yDeXFfEf0FA",
        "outputId": "47f183f6-66b5-4537-af41-bf5f5c502e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1358\n",
            "3463\n",
            "Rand Index: 0.2154149163796027\n",
            "Accuracy: 0.7356406480117821\n",
            "Adjusted Rand Index: 0.2154149163796027\n",
            "Fowlkes Mallows Score: 0.4752907084015942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    }
  ]
}